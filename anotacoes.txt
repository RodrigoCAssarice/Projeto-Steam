# 1) Ir para o diretório onde deseja criar o projeto
cd $env:USERPROFILE\Documents

# 2) Criar a pasta raiz
New-Item -ItemType Directory -Path steam-data-pipeline | Out-Null
cd steam-data-pipeline

# 3) Criar estrutura principal
$dirs = @(
  "src/functions/orchestrators",
  "src/functions/triggers",
  "src/functions/shared",
  "src/collectors/steam",
  "src/collectors/ecommerce",
  "src/streaming/producers",
  "src/streaming/consumers",
  "src/streaming/schemas",
  "src/parsers/models",
  "src/parsers/extractors",
  "src/processing/bronze",
  "src/processing/silver",
  "src/processing/gold",
  "src/processing/common",
  "src/dlt/pipelines",
  "src/dlt/expectations",
  "src/bi/dimensional",
  "src/bi/dashboards",
  "src/monitoring/grafana",
  "src/monitoring/logs",
  "src/monitoring/metrics",
  "infra/terraform",
  "infra/bicep",
  "tests/unit",
  "tests/integration",
  "docs",
  ".github/workflows",
  "scripts",
  "config/dev",
  "config/prod"
)

$dirs | ForEach-Object { New-Item -ItemType Directory -Path $_ | Out-Null }

# 4) Criar arquivos iniciais
New-Item -ItemType File -Path README.md | Out-Null
New-Item -ItemType File -Path .gitignore | Out-Null
New-Item -ItemType File -Path pyproject.toml | Out-Null
New-Item -ItemType File -Path requirements.txt | Out-Null
New-Item -ItemType File -Path config/dev/.env | Out-Null
New-Item -ItemType File -Path config/prod/.env | Out-Null

# 5) Iniciar repositório Git
git init

verificar se criou a estrutura corretamente
Get-ChildItem -Recurse | Select-Object FullName




Tarefa: criar venv, dependências e arquivos de configuração
- Crie e ative o ambiente virtual (PowerShell):
cd C:\Users\Rodrigo\Desktop\py\prjt\steam-data-pipeline
python -m venv .venv
.\.venv\Scripts\Activate.ps1
- Atualize o pip e instale dependências essenciais:
python -m pip install --upgrade pip
pip install azure-functions requests python-dotenv pydantic
- Preencha o arquivo requirements.txt na raiz com:
azure-functions==1.20.0
requests==2.32.3
python-dotenv==1.0.1
pydantic==2.9.2
- Crie o arquivo host.json na raiz do projeto com:
{
  "version": "2.0",
  "logging": {
    "logLevel": {
      "Function": "Information"
    }
  },
  "extensions": {
    "http": {
      "routePrefix": ""
    },
    "timers": {
      "scheduleMonitor": {
        "enabled": true
      }
    }
  }
}
- Crie o arquivo local.settings.json na raiz (não commitar em prod):
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
    "FUNCTIONS_WORKER_RUNTIME": "python",
    "ENV": "dev",
    "STEAM_API_BASE": "https://store.steampowered.com",
    "CAPTURE_TARGETS": "steam"
  }
}



Tarefa: scaffold da Function de timer
- Crie a pasta da function de timer:
- src/functions/triggers/capture_daily
- Crie o arquivo src/functions/triggers/capture_daily/function.json:
{
  "scriptFile": "__init__.py",
  "bindings": [
    {
      "name": "timer",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 0 6 * * *"
    }
  ]
}
- Explicação curta: cron “0 0 6 * * *” roda todo dia às 06:00 BRT. Ajustaremos depois se precisar.
- Crie o arquivo src/functions/triggers/capture_daily/__init__.py:
import os
import json
import datetime as dt
import azure.functions as func
from pathlib import Path
from dotenv import load_dotenv
import requests

load_dotenv(dotenv_path=Path("config/dev/.env"))

def _now_iso():
    return dt.datetime.now(dt.timezone.utc).isoformat()

def _capture_steam_featured():
    base = os.getenv("STEAM_API_BASE", "https://store.steampowered.com")
    url = f"{base}/api/featuredcategories"
    headers = {"User-Agent": "steam-data-pipeline/1.0"}
    r = requests.get(url, timeout=30, headers=headers)
    r.raise_for_status()
    payload = r.json()
    return {
        "source": "steam",
        "endpoint": "featuredcategories",
        "captured_at": _now_iso(),
        "data": payload
    }

def main(timer: func.TimerRequest) -> None:
    # Decide alvos de captura
    targets = os.getenv("CAPTURE_TARGETS", "steam").split(",")

    results = []
    if "steam" in [t.strip() for t in targets]:
        try:
            results.append(_capture_steam_featured())
        except Exception as e:
            results.append({
                "source": "steam",
                "error": str(e),
                "captured_at": _now_iso()
            })

    # Persistência inicial em arquivo local (bronze/raw local)
    out_dir = Path("src/processing/bronze")
    out_dir.mkdir(parents=True, exist_ok=True)
    outfile = out_dir / f"raw_{dt.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with outfile.open("w", encoding="utf-8") as f:
        json.dump({"items": results}, f, ensure_ascii=False, indent=2)

    print(f"[capture_daily] wrote {outfile}")
- Atualize .gitignore para evitar sujeira local:
.venv/
__pycache__/
local.settings.json
*.log
.python_packages/



Teste: validar execução local
- Pré-requisito: Você já tem Azure Functions Core Tools instalado? Se não tiver, me avisa que te passo o passo a passo.
- Execute no terminal (com venv ativo):
func start
- O que esperar:
- Logs mostrando o carregamento das functions.
- Execução automática do trigger de timer ao iniciar (ou manual pelo painel, “Run”).
- Mensagem “wrote src/processing/bronze/raw_YYYYMMDD_HHMMSS.json”.
- Arquivo JSON criado em src/processing/bronze com um objeto items contendo a resposta da Steam ou um erro registrado.

Checkpoint e próximos passos
- Confirme “ok” após ver o arquivo JSON criado com conteúdo.
- Na Etapa 3, vamos criar um coletor dedicado (módulo src/collectors/steam) com funções robustas, contratos de schema em src/streaming/schemas, e integração com Storage Account (Blob) para escrita incremental. Quer que a gravação já vá para Azure Blob na próxima etapa?




//"schedule": "0 0 6 * * *"




Meta: Captação de Dados Brutos em Quase Tempo Real: coletar dados de jogos da Steam e preços
Objetivo: Criar uma aplicação no Azure Functions para capturar dados brutos diários de sites de e-commerce e marketplaces para análise posterior. Benefício: Permite produzir dados em quase tempo real com menor custo. Ingestão de dados em Streaming:
Objetivo: Criar consumidor streaming de forma incremental no storageaccount. Benefício: Permite a captura de dados em streaming utilizando recurso interátivo na Azure de baixa compelxidade e de menor custo. Escalando Captura de Elemento HTML:
Objetivo: Utilizar modelos de IA para extrair informações essenciais dos arquivos HTML, como links, títulos, preços, promoções, parcelamentos e imagens dos produtos. Benefício: Escalabilidade para ampliar o leque de fonte de dados e flexibilidade para processo produtivo com menor probabilidade para correções de bugs. Processamento e Escalabilidade para BigData:
Objetivo: Aplicar processamento Spark ao conjuntos de dados na arquitetura medalhão do pipeline. Benefício: Melhoria da qualidade dos dados, com correção de dados e padronização do mesmo e, além disso, o processamento de dados escalonáveis e suportando big data. Data Lakehouse Estrutura Medalhão de Dados:
Objetivo: Transformar o datalake (storageaccount) em um data lakehouse. Benefício: Combinando a flexibilidade e escalabilidade de um data lake com a confiabilidade e performance de um data warehouse. Pipeline ELT com DLT:
Objetivo: Criar um pipeline de dados com baixa complexidade de desenvolvimento e com grandes ganho em qualidade e monitoramento de dados. Benefício: o DLT transforma pipelines de ETL/ELT em algo mais confiável, menos manual e pronto para escalar, acelerando a entrega de valor com dados. Otimizando o processo para análise e BI:
Objetivo: Utilização do Spark SQL no Databricks para a criação de tabelas fato e dimensão a partir de uma tabela Gold do Data Lakehouse. Benefício: Permite uma modelagem de dados analíticos eficiente e confiável, combinando a familiaridade do SQL com a escalabilidade do Spark e a integridade do Delta Lake. Monitoramento de Processos:
Objetivo: Monitoramento de todos recurtsos e status do projeto. Benefício: Monitoramento de todas as funcionalidades e recursos da arquitetura do projeto em um único local (Grafana).



 “Agora com todos os dados vamos continuar"

